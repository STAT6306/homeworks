---
title: "Solution 2"
subtitle: 'STAT6306'
output: pdf_document
---

#Introduction

A major issue with antiretroviral drugs is the mutation of the virus' genes.  Because of its high rate of replication ($10^9$ to $10^{10}$ virus per person per day) and error-prone
polymerase\footnote{An enzyme that `stitches' back together DNA or RNA after replication}, HIV can 
easily develop mutations that alter susceptibility to antiretroviral drugs.
The emergence of resistance to one or more antiretroviral drugs is one of the more
common reasons for therapeutic failure in the treatment of HIV.

In the paper 'Genotypic predictors of human immunodeficiency virus type 1 drug resistance'\footnote{The entire paper is on the website.  Try to see what you can get out of it if you have the time.}, 
a sample of in vitro\footnote{Latin for `in glass', sometimes known colloquially as a test tube}
HIV viruses were grown and exposed to a particular antiretroviral therapy.  The susceptibility of the virus to treatment
and the number of genetic mutations of each virus were recorded.


#Question 1

```{r}
load("hiv.rda")

X = hiv.train$x
Y = hiv.train$y

geneLabels = colnames(X)
```


## (a)  
What are $n$ and $p$ in this problem?  What are the features in this problem?  What are the observations? What is the supervisor? \textbf{Note:} Attempt to answer this question before moving on to the rest of the questions.

```{r}
#SOLUTION
(n = nrow(X))
(p = ncol(X))
```

### SOLUTION
There are 208 features (p) and 704 observations (n). The features are indicators for whether or not there was a mutation in a gene.  The supervisor is the log(susceptibility) of the HIV virus to a particular drug therapy

#Question 2
Consider the feature matrix $\mathbb{X}$.  It is composed of 0's and 1's, with a 1 indicating a mutation in a particular gene.  Look at the output
for the following chunk of code.
```{r}
table(X)
```
What results do you see?  What does this indicate?

## SOLUTION
Based on the feature matrix X, we see that there are 135589 unmutated/"normal" genes and 10843 genes that have mutations.

#Question 3
The supervisor is the log transformed susceptibility of a virus to the considered treatment, with large values
indicating the virus is relatively more resistant (that is, not susceptible).  Run
```{r}
hist(Y)
```

What plot did you just create?  What does this indicate?

## SOLUTION
This gives us a histogram of the frequency of the susceptibility of a virus to the considered treatment.  The marginal distribution of the supervisor
Y is bimodal, with a peak around 0 (higher susceptible) and around 2.4 (lower susceptible)

#Question 4
We may have (at least) two goals with a data set such as this: 

* inference: can we find some genes whose mutation seems to be most related to viral susceptibility?
* prediction: can we make a model that would predict whether this therapy would be efficacious, given a virus 
with a set of genetic mutations

## (a) 
Try to find the best subset solution
for this problem. Discuss any problems or findings you discover.  In particular, 
how many possible models are there?  

### SOLUTION
```{r}
2^p
```

There are $2^p$ possible different solutions, which, given the size of p (208), gives us 4.1137614e+62 possible solutions, which is way too large to compute all subsets.

## (b) Inference

### (i) 
Find the selected model for:

* forward selection using BIC as the criterion
* lasso
* refitted/relaxed lasso

```{r}
#Forward selection
#SOLUTION
if(!require(leaps)){install.packages('leaps',repos='http://cran.us.r-project.org');require(leaps)}
outForward      = regsubsets(x=X,y=Y,nvmax=p,method='forward')
#  note this warning is that the feature matrix
#  isn't full rank.  This is, there are redundant
#  columns in it:
cat('The rank is: ',qr(X)$rank,' while the # of features is: ',p,'\n')

sumForward      = summary(outForward)
model.forward   = sumForward$which[which.min(sumForward$bic),]
S.forward       = model.forward[-1]#get rid of the intercept entry
lm.forward      = lm(Y~X[,S.forward])#regsubsets only scores models, not fit them
betaHat.forward = coef(lm.forward)
betaHat.forward
```

```{r}
#lasso
if(!require(glmnet)){install.packages('glmnet',repos='http://cran.us.r-project.org');require(glmnet)}
#SOLUTION
lasso.cv.glmnet = cv.glmnet(X,Y,alpha=1)#note: we are standardizing the features...
plot(lasso.cv.glmnet) #note that this output is random... why?
betaHat.lasso   = coef(lasso.cv.glmnet,s='lambda.min')[-1]
S.lasso         = which(abs(betaHat.lasso) > 1e-16)
```

```{r}
#refitted/relaxed lasso
#SOLUTION
lasso.cv.glmnet  = cv.glmnet(X,Y,alpha=1)
plot(lasso.cv.glmnet)
betaHat.temp     = coef(lasso.cv.glmnet,s='lambda.1se')[-1]
S.refitted       = which(abs(betaHat.temp) > 1e-16)
lm.refitted      = lm(Y ~ X[,S.refitted])
betaHat.refitted = coef(lm.refitted)
```

### (ii) 
Comparing the selected models for each of the above methods

```{r}
#SOLUTION
cat('The selected genes from forward selection + BIC are: \n',
     geneLabels[S.forward],'\n')

cat('The selected genes from lasso are: \n',
     geneLabels[S.lasso],'\n')

cat('The selected genes from refitted lasso are: \n',
     geneLabels[S.refitted],'\n')

#Note that we can directly compare chosen models
geneLabels[S.forward]  %in% geneLabels[S.refitted]
geneLabels[S.refitted] %in% geneLabels[S.forward]
```

## (c) Prediction

###(i) Ridge regression
Now that are looking at prediction, we can use ridge regression (which only addresses prediction).  Using the package glmnet, plot the
CV curve over the grid of $\lambda$ values and indicate the minimum, and finally report the CV
estimate of the prediction risk for $\hat\beta_{\textrm{ridge}}(\hat\lambda)$

\textbf{Note:}  There is no need to report the $p$ coefficient estimates from the ridge solution.
Also, glmnet has a grid problem.  Make two plots, one that shows the problem and one that shows it being corrected.  

```{r}
#SOLUTION
ridge.cv.glmnet = cv.glmnet(X,Y,alpha=0)
plot(ridge.cv.glmnet)
#or
plot(ridge.cv.glmnet$lambda,ridge.cv.glmnet$cvm,type='l')
#CV estimate of the prediction error: 
min(ridge.cv.glmnet$cvm)

min.lambda      = min(ridge.cv.glmnet$lambda)
lambda.new      = seq(min.lambda, min.lambda*0.01,length=100)
ridge.cv.glmnet = cv.glmnet(x = X, y = Y, alpha = 0,lambda = lambda.new)
plot(ridge.cv.glmnet) #now it is in middle

```

### (ii) Prediction on a test set
Now, let's look at some predictions made by these methods.  Use the following for the test set:
```{r}
X_0 = hiv.test$x
Y_0 = hiv.test$y
```

Find an estimate of the risk using the test observations for

* forward selection using BIC as the criterion
* ridge
* lasso
* refitted/relaxed lasso
```{r}
#SOLUTION
#### Get predictions on test set:
Yhat.test.forward  = X_0[,S.forward] %*% betaHat.forward[-1] + betaHat.forward[1]
Yhat.test.ridge    = predict(ridge.cv.glmnet,X_0,s='lambda.min')
Yhat.test.lasso    = predict(lasso.cv.glmnet,X_0,s='lambda.min')
Yhat.test.refitted = X_0[,S.refitted] %*% betaHat.refitted[-1] + betaHat.refitted[1]

# Get estimate of prediction risk via the test set error
Yhat.test.forward   = mean((Yhat.test.forward - Y_0)**2)
pred.error.ridge    = mean((Yhat.test.ridge - Y_0)**2)
pred.error.lasso    = mean((Yhat.test.lasso - Y_0)**2)
pred.error.refitted = mean((Yhat.test.refitted - Y_0)**2)

cat('The prediction error from forward selection + BIC are: \n',
     Yhat.test.forward,'\n')

cat('The prediction error from ridge is: \n',
     pred.error.ridge,'\n')

cat('The prediction error from lasso is: \n',
     pred.error.lasso,'\n')

cat('The prediction error from refitted lasso is: \n',
     pred.error.refitted,'\n')
```

### (d)
\textbf{Challenge} Suppose we didn't have access to any test data.  How could you provide an estimate
of the risk?  What are the pros and cons of your proposal?
```{r}
#SOLUTION
cat('The CV estimate of risk of ridge(lambdaHat) = ',min(ridge.cv.glmnet$cvm),'\n')
```
Compare this with the test set estimate: 
```{r}
cat('The test set estimate of risk from ridge is: \n',
     pred.error.ridge,'\n')
```
So, by minimizing CV as a function of lambda, we have produced a reasonable, but overly optimistic estimate of the risk.

#Question 6
Using the lasso with CV minimum tuning parameter, which gene mutations are related to susceptibility? 
```{r}
#SOLUTION

geneLabels[S.lasso]
```

#Question 7
At which gene mutation sites are the presence of a mutation associated with a decrease in viral susceptibility to this particular drug? Hint: Consider the signs of the coefficients. What gene site has the largest estimated effect using $\hat\beta_{\textrm{lasso}}(\hat\lambda)$?
```{r}
#SOLUTION
## Gene mutation sites related to a decrease in viral susceptibility:
##  thought process: Y increase <-> decrease susceptibility (i.e. increase resistance)
##                  so \hat{beta}_j > 0 associated with decrease in susceptibility

geneLabels[betaHat.lasso > 1e-16]
geneLabels[which.max(abs(betaHat.lasso))]
```


# Additional challenge problems:
I don't want to overwhelm you with homework problems.  However, there are additional topics that are relevant for an interested student.  You don't need
to do these/turn them in.

#Question 8
Derive, implement, and run both "batch" and "stochastic" gradient descent for this HIV data.

#Question 9
The LARS algorithm is quite similar to forward selection.  Run LARS using the option forward.stagewise and compare it to forward selection using Mallow's Cp.

#Question 10
Try and use a GIC-based method instead of K-fold CV for finding $\hat\lambda$ using the HIV data.



