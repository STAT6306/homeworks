---
title: "Homework 3"
subtitle: 'STAT6306; Due: 10/17/2017 before class'
output: pdf_document
---

#Introduction


For this assignment, let's attempt to make a spam filter.
Usually, this would involve a lot of text processing on a huge number of emails.  In this case, someone has created a 
feature matrix for us.  The feature matrix has rows given by individual emails and columns given by the number of each word or character that appears 
in that email, as well as three different numerical measures regarding capital letters (average
length of consecutive capitals, longest sequence of consecutive capitals, and total number of capital letters).   
The supervisor, Y, is given by the user supplied label marking that email as either spam (Y = 1) or not (Y = 0).
Here is a function that may be useful for this assignment:

```{r}
misClass =function(pred.class,true.class,produceOutput=FALSE){
  confusion.mat = table(pred.class,true.class)
  if(produceOutput){
    return(1-sum(diag(confusion.mat))/sum(confusion.mat))	
  }
  else{
    print('miss-class')
    print(1-sum(diag(confusion.mat))/sum(confusion.mat))
    print('confusion mat')
    print(confusion.mat)
  }
}
# this can be called using: 
#     (assuming you make the appropriately named test predictions)
# misClass(Y.hat,Y_0)
```

# Question 1
Read in the R data set spam.Rdata (note: when the object is an Rdata file, use load(spam.Rdata) 
and read the documentation file spambase.Documentation. 

```{r}
##SOLUTION
```
 What object(s) is/are loaded into memory?  What objects are inside that object?  How many
emails do we have total?  What features are in this data set?  

## SOLUTION


# Question 2
Let's make a training and test set. 
```{r}
train = spam$train
test  = !train
X     = spam$XdataF[train,]
X_0   = spam$XdataF[test,]
Y     = factor(spam$Y[train])
Y_0   = factor(spam$Y[test])
```

How many observations are in the training set (that is, what is $n$)?  How many observations are in the test set?

## SOLUTION

# Question 3
Read the document about using SQL and R (Rsql.Rmd and Rsql.pdf).  Use the same ideas to create a SQL database
that has 4 tables, one for each of the training/test feature/supervisor.

Once this database has been created, read in the information back into R from SQL.  Include your code here
for these steps. 
```{r}
## SOLUTION
write.csv(X,file='Xtrain.csv',row.names = FALSE)
write.csv(X_0,file='Xtest.csv',row.names = FALSE)
write.csv(Y,file='Ytrain.csv',row.names = FALSE)
write.csv(Y_0,file='Ytest.csv',row.names = FALSE)

#Get ride of objects from R's memory as you will be reloading them
#from the database you will be creating
rm(X);rm(X_0);rm(Y);rm(Y_0)


if(!require('sqldf')){install.packages('sqldf', repos = 'http://cran.us.r-project.org'); require('sqldf')}

db = dbConnect(SQLite(), dbname='spam.sqlite')

X = read.csv(file='Xtrain.csv')
dbWriteTable(conn = db, name = 'Xtrain', 
             value = X, row.names = FALSE,
             col.names = TRUE,overwrite=TRUE)
rm(X)
#print out first few rows/columns
dbReadTable(db,'Xtrain')[1:5,1:5]

X              = dbReadTable(db,'Xtrain')
###CONTINUE ON FROM HERE...
```

#Question 4
Let's build a classifier using logistic lasso.  Report the selected features and identify which ones are associated with an
increase in the estimated probability of being spam and which ones are associated with a decrease in the estimated probability
of being spam via a ``word cloud'' visualization.  I have included some of the relevant code below.  Complete the code (including
installing wordcloud).

```{r}
require(glmnet)
Xmat = as.matrix(X,dimnames = NULL)
Ynum = as.numeric(Y)-1

lasso.cv.glmnet = cv.glmnet(Xmat,Ynum,alpha=1,family='binomial')

##SOLUTION
# Check that the CV minimum doesn't occur on boundary
# Get the coefficient estimate, call it betaHat.lasso
# Get the selected model, call it S.lasso

require(wordcloud)
negPos = betaHat.lasso[S.lasso] < 0
colVec = rep('red',length(negPos))
colVec[negPos] = 'blue'

wordcloud(names(X)[S.lasso],abs(betaHat.lasso[S.lasso]), 
          min.freq = 1e-16, scale=c(4,.5),
          colors = colVec,ordered.colors=T,random.order=T, rot.per=.15)
```

Explain the meaning of the color and size of the words in this plot.  Which word is most associated with the email being spam?
Which word is least associated with being spam (that is, most associated with not being spam)?

##SOLUTION

#Question 5: LDA
Run the following code
```{r}
require(MASS)
out.lda = lda(Y~.,data=X)
out.lda$prior
```
What do these `prior' values correspond to?

##SOLUTION 

As mentioned in lecture, LDA provides supervised dimension reduction
as well.  We can visualize this dimension reduction via plotting the lda object.  Recall that the dimension reduction is to a dimension 1 less than the number of levels of the supervisor.  In this case, there are 2 levels (spam or no spam) and hence it is 1 dimensional. In this case, R plots two histograms: one for training observations with supervisor `spam' and one for training observations with supervisor 'not spam'.  Insights into the data can be made by noticing that the emails that are near to zero are more difficult to classify (there is a nice discussion of LDA in R here: http://uc-r.github.io/discriminant_analysis).
```{r}
plot(out.lda)
```

Let's try the following just for fun.  Let's make a third level
for the supervisor.  Looking at the `posterior' probability estimates,
any probability near 0.5 indicates that we are relatively unsure of
the classification.  So, let's make the third level those observations
with probabilities between 0.4 and 0.6
```{r}
pred.lda = predict(out.lda)
unsure = pred.lda$posterior[,1] > 0.4 & pred.lda$posterior[,1] < 0.6
Ynew = Ynum
Ynew[unsure] = 2
table(Ynew)
out.lda = lda(Ynew~.,data=X)
plot(out.lda,col=Ynew+1)
```

Comment on this new plot.  In particular, what dimension is it in
and what seems to be the relationship between the levels of the supervisor?

##SOLUTION: 



#Question 6: Trees
Run the following code
```{r}
if(!require('tree')){install.packages('sqldf', repos = 'http://cran.us.r-project.org'); require('tree')}
out.tree = tree(Y~.,data=X)
tmp.tree = prune.tree(out.tree,best=3)
plot(tmp.tree)
text(tmp.tree)
```

Interpet the first split point/feature 

##SOLUTION

Make the corresponding partition view for this dendrogram 

```{r}
#SOLUTION
#hint, you can use the "plot command to do this.
#Here is the code I used to make the plot on tree slides 6 & 7:

#color = rep('blue',nrow(X))
#color[X$Maine < 0.113708 & X$sp500 < -0.0313227] = 'orange'
#plot(X$Maine,X$sp500,xlab='Maine',ylab='S & P 500',cex=.4,col=color)
#abline(h = -0.0313227) #S & P 500
#segments(x0 = 0.113708, y0 = min(X$sp500-.5), y1 = -0.0313227) #MAINE
#text(x = c(-2,-2,1.5),y = c(-0.2,0.2,-0.2), #labels=c('R1','R2','R3'))
```

#Question 7: Misclassification rates and confusion matrices
Fit an unpruned classification tree to the training data (hint: you've already done that on this h/w).
Get the associated test misclassification rate and test confusion matrix.  Compare this to the misclassification
rate and test confusion matrices for the logistic lasso.

```{r}
# Get the test predictions via logistic lasso
Xmat_0 = as.matrix(X_0,dimnames = NULL)
Ynum_0 = as.numeric(Y_0)-1
Yhat.lasso   = Yhat.lasso = predict(lasso.cv.glmnet, Xmat_0,
                     s='lambda.min',type='class')
Yhat.tree    = predict(out.tree,X_0,type='class')
##SOLUTION: Use the misclass function
```