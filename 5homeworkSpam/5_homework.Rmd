---
title: "Homework 5"
subtitle: 'STAT6306: Due Nov. 21 at 11:59 pm'
output: pdf_document
---

#Introduction


For this assignment, let's attempt to make a spam filter.
Usually, this would involve a lot of text processing on a huge number of emails.  In this case, someone has created a 
feature matrix for us.  The feature matrix has rows given by individual emails and columns given by the number of each word or character that appears 
in that email, as well as three different numerical measures regarding capital letters (average
length of consecutive capitals, longest sequence of consecutive capitals, and total number of capital letters).   
The supervisor, Y, is given by the user supplied label marking that email as either spam (Y = 1) or not (Y = 0).
Here is a function that may be useful for this assignment:

```{r}
misClass =function(pred.class,true.class,produceOutput=FALSE){
  confusion.mat = table(pred.class,true.class)
  if(produceOutput){
    return(1-sum(diag(confusion.mat))/sum(confusion.mat))	
  }
  else{
    print('misClass')
    print(1-sum(diag(confusion.mat))/sum(confusion.mat))
    print('confusion mat')
    print(confusion.mat)
  }
}
# this can be called using: 
#     (assuming you make the appropriately named test predictions)
# misClass(Y.hat,Y_0)
```


Read in the R data set:

```{r}
load("spam.Rdata")
```
 

Let's make a training and test set. 
```{r}
train = spam$train
test  = !train
X     = spam$XdataF[train,]
X_0   = spam$XdataF[test,]
Y     = spam$Y[train]
Y_0   = spam$Y[test]
```

# Install necessary packages
```{r}
repos = 'http://cran.us.r-project.org'
packages = c('randomForest','gbm','e1071')
for(package in packages){
  if(!require(package,character.only=TRUE)){
    install.packages(package,repos = repos)
    require(package,character.only=TRUE)
  }
}
```

# Question 1: Boosting with gbm
There are 3 main choices involved in boosting trees: the number of boosting iterations, the learning rate, and the tree complexity [see list at end of 8.2.3 of ISL for a brief discussion].
Using the gbm package, explore a variety of values for these parameters and note the test confusion matrices.  In your write-up of this
problem, I want you to just record your observations about the effects of these parameters.
Report the best values.  Do your conclusions change depending on if you use Adaboost versus
Bernoulli loss?


```{r}
lambdaGrid            = c(.01,.1,.25)
interaction.depthGrid = c(4,5,6)
n.treesGrid           = c(500,1000,2000)
distribution          = 'bernoulli'
set.seed(1)
verbose = 0
for(lambda in lambdaGrid){
  for(interaction.depth in interaction.depthGrid){
    for(n.trees in n.treesGrid){
      boost.out = gbm(Y~.,data=X,
                      n.trees=n.trees, interaction.depth=interaction.depth,
                      shrinkage=lambda, distribution = distribution)
      prob.hat  = predict(boost.out,X_0,n.trees=n.trees,type='response')
      Y.hat = rep(0,nrow(X_0))
      Y.hat[prob.hat > 0.5] = 1
      Y.hat = as.factor(Y.hat)
      if(verbose > 0){
      cat('lambda = ',lambda,' interaction.depth = ',interaction.depth, ' lambda = ',lambda,' n.trees = ',n.trees,'\n')
      }
      if(verbose > 1){
        misClass(Y.hat,Y_0)      
      }
    }
  }
}

##SOLUTION: put in values for interaction.depth and shrinkage based on above output
interaction.depth = ?
shrinkage         = ?
#Let's look at a grid of B's at these values:
n.treesGrid = c(2,50,100,300,500,1000,2000,3000,5000)
results = rep(0,length(n.treesGrid))
set.seed(1)
boost.out = gbm(Y~.,data = X, n.trees = max(n.treesGrid),
                  interaction.depth = interaction.depth,
                  shrinkage = shrinkage,
                  distribution = distribution)

getClassesF = function(n.trees,features,supervisor){
  prob.hat  = predict(boost.out,features,n.trees=n.trees,type='response')
  Y.hat = rep(0,nrow(features))
  Y.hat[prob.hat > 0.5] = 1
  return(misClass(Y.hat,supervisor,produceOutput=TRUE))
}
results = sapply(n.treesGrid,getClassesF,features=X_0,supervisor=Y_0)

##SOLUTION: Make a plot displaying these test misclassification rates as a function of B
```

#Question 2: Boosting vs. Random Forest
Make a plot of the training misclassifications and
test misclassifications for a grid of B values from 3 up to 5000 (it doesn't have to be a dense grid, just choose a representative set of values) for both bagging and boosting (so, one plot with 4 curves).  Which method gets the 
lowest training error? The lowest test error?
```{r}
##SOLUTION
```

#Question 3: Boosting importance

Just like in bagging, we can get an idea about variable importance.  For each tree in the boosting
ensemble, and for each feature, we record the amount the training misclassification rate is reduced by making 
a split on the $j^{th}$ feature.  We take the average of this over all B trees to get the importance.  We
can have R give us this information from running summary on the output of gbm.  What are the 3
most important variables?

```{r}
##SOLUTION
summary(boost.out,n.trees=?)
```



#Question 4: SVMs
Let's look at SVMs for fitting the spam data.
Find the (test) confusion matrix for an SVM with nonlinear kernel.  Choose the cost parameter and the kernel parameters
with cross-validation. 

```{R}
Y      = as.factor(Y)
df     = data.frame(X=X, Y=Y)
testDf = data.frame(X=X_0,Y=Y_0)
out = tune(svm,Y~.,data=df,kernel="linear",type='C',
           ranges=list(cost=10^seq(-3,0)))
linear.cv = out$best.performance
out.linear = out$best.model
Yhat.linear = predict(out.linear,newdata=testDf)
print('Linear')
cMat = misClass(Yhat.linear,Y_0)
cat('Cv est: ',linear.cv)
cMat[1,1]/sum(cMat[,1])
cMat[2,2]/sum(cMat[,2])
#SOLUTION: Now do a nonlinear kernel SVM:
```
